# -*- coding: utf-8 -*-
"""Mini_project_q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zhh8hpvJaCJ6q_DEXBGWCggvO_pkGXVV

practice number one (have 5 step)
step 1:
"""

import numpy as np

# Setting the random state
np.random.seed(11)

from sklearn.datasets import make_classification

# Regenerating the dataset with adjusted parameters
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, random_state=11)

X.shape, y.shape

"""step2:"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)

# Initializing models with default parameters
log_reg = make_pipeline(StandardScaler(), LogisticRegression(random_state=11))
svm = make_pipeline(StandardScaler(), SVC(random_state=11))

# Training the Logistic Regression model
log_reg.fit(X_train, y_train)
log_reg_train_acc = accuracy_score(y_train, log_reg.predict(X_train))
log_reg_test_acc = accuracy_score(y_test, log_reg.predict(X_test))

# Training the SVM model
svm.fit(X_train, y_train)
svm_train_acc = accuracy_score(y_train, svm.predict(X_train))
svm_test_acc = accuracy_score(y_test, svm.predict(X_test))

(log_reg_train_acc, log_reg_test_acc, svm_train_acc, svm_test_acc)

"""step3:"""

import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np

def plot_decision_boundaries_corrected(X, y, classifier, test_idx=None, title="Decision Boundary"):
    # Setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # Plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),
                           np.arange(x2_min, x2_max, 0.02))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    # Plot all samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
                    alpha=0.8, c=colors[idx],
                    marker=markers[idx], label=f"Class {cl}")

    # Highlight test samples and misclassified samples
    if test_idx:
        # Extract test samples
        X_test, y_test = X[test_idx, :], y[test_idx]
        y_pred = classifier.predict(X_test)
        misclassified = y_pred != y_test

        # Marking test samples
        plt.scatter(X_test[:, 0], X_test[:, 1],
                    c='none', edgecolor='black',
                    linewidth=1, marker='o', s=100,
                    label='Test Set')

        # Marking misclassified samples
        plt.scatter(X_test[misclassified, 0], X_test[misclassified, 1],
                    facecolors='none', edgecolor='yellow',
                    marker='o', s=100, linewidth=2,
                    label='Misclassified')

    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend(loc='upper left')

# Assuming X_train and X_test have been concatenated
# Define test_idx as the range of indices for the test set
test_idx = range(len(X_train), len(X_train) + len(X_test))

# Now you can call the plotting function
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plot_decision_boundaries_corrected(X=np.vstack((X_train, X_test)), y=np.hstack((y_train, y_test)),
                         classifier=log_reg, test_idx=test_idx,
                         title="Decision Boundary - Logistic Regression")

plt.subplot(1, 2, 2)
plot_decision_boundaries_corrected(X=np.vstack((X_train, X_test)), y=np.hstack((y_train, y_test)),
                         classifier=svm, test_idx=test_idx,
                         title="Decision Boundary - SVM")

plt.tight_layout()
plt.show()

"""step4:"""

# Generating a more challenging dataset with increased overlap between classes
X_challenging, y_challenging = make_classification(n_samples=1000, n_features=2, n_informative=2,
                                                   n_redundant=0, n_repeated=0, n_classes=2,
                                                   class_sep=0.5,  # Reduced class separation
                                                   random_state=11)

# Splitting the new dataset into training and testing sets
X_train_chal, X_test_chal, y_train_chal, y_test_chal = train_test_split(X_challenging, y_challenging, test_size=0.2, random_state=11)

# Training the Logistic Regression model on the new dataset
log_reg_chal = make_pipeline(StandardScaler(), LogisticRegression(random_state=11))
log_reg_chal.fit(X_train_chal, y_train_chal)
log_reg_train_acc_chal = accuracy_score(y_train_chal, log_reg_chal.predict(X_train_chal))
log_reg_test_acc_chal = accuracy_score(y_test_chal, log_reg_chal.predict(X_test_chal))

# Training the SVM model on the new dataset
svm_chal = make_pipeline(StandardScaler(), SVC(random_state=11))
svm_chal.fit(X_train_chal, y_train_chal)
svm_train_acc_chal = accuracy_score(y_train_chal, svm_chal.predict(X_train_chal))
svm_test_acc_chal = accuracy_score(y_test_chal, svm_chal.predict(X_test_chal))

(log_reg_train_acc_chal, log_reg_test_acc_chal, svm_train_acc_chal, svm_test_acc_chal)
# Indices of the test set for the challenging dataset
test_idx_chal = range(len(y_train_chal), len(y_train_chal) + len(y_test_chal))

# Plot for Logistic Regression on challenging dataset
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plot_decision_boundaries_corrected(X=np.vstack((X_train_chal, X_test_chal)), y=np.hstack((y_train_chal, y_test_chal)),
                         classifier=log_reg_chal, test_idx=test_idx_chal,
                         title="Decision Boundary - Logistic Regression (Challenging Dataset)")

# Plot for SVM on challenging dataset
plt.subplot(1, 2, 2)
plot_decision_boundaries_corrected(X=np.vstack((X_train_chal, X_test_chal)), y=np.hstack((y_train_chal, y_test_chal)),
                         classifier=svm_chal, test_idx=test_idx_chal,
                         title="Decision Boundary - SVM (Challenging Dataset)")

plt.tight_layout()
plt.show()

"""step5:"""

# Generating a dataset with 3 classes and 1 cluster per class
X_3_classes, y_3_classes = make_classification(n_samples=1000, n_features=2, n_informative=2,
                                               n_redundant=0, n_repeated=0, n_classes=3,
                                               n_clusters_per_class=1, random_state=11)

# Splitting the dataset into training and testing sets
X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3_classes, y_3_classes, test_size=0.2, random_state=11, stratify=y_3_classes)

# Training the Logistic Regression model for 3 classes
log_reg_3 = make_pipeline(StandardScaler(), LogisticRegression(random_state=11, multi_class='multinomial'))
log_reg_3.fit(X_train_3, y_train_3)
log_reg_train_acc_3 = accuracy_score(y_train_3, log_reg_3.predict(X_train_3))
log_reg_test_acc_3 = accuracy_score(y_test_3, log_reg_3.predict(X_test_3))

# Training the SVM model for 3 classes
svm_3 = make_pipeline(StandardScaler(), SVC(random_state=11, decision_function_shape='ovo'))
svm_3.fit(X_train_3, y_train_3)
svm_train_acc_3 = accuracy_score(y_train_3, svm_3.predict(X_train_3))
svm_test_acc_3 = accuracy_score(y_test_3, svm_3.predict(X_test_3))

(log_reg_train_acc_3, log_reg_test_acc_3, svm_train_acc_3, svm_test_acc_3)
# Indices of the test set for the 3-class dataset
test_idx_3 = range(len(y_train_3), len(y_train_3) + len(y_test_3))

# Plot for Logistic Regression on 3-class dataset
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plot_decision_boundaries_corrected(X=np.vstack((X_train_3, X_test_3)), y=np.hstack((y_train_3, y_test_3)),
                         classifier=log_reg_3, test_idx=test_idx_3,
                         title="Decision Boundary - Logistic Regression (3 Classes)")

# Plot for SVM on 3-class dataset
plt.subplot(1, 2, 2)
plot_decision_boundaries_corrected(X=np.vstack((X_train_3, X_test_3)), y=np.hstack((y_train_3, y_test_3)),
                         classifier=svm_3, test_idx=test_idx_3,
                         title="Decision Boundary - SVM (3 Classes)")

plt.tight_layout()
plt.show()
#done